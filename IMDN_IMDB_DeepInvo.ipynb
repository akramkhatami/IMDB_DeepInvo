{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akramkhatami/IMDB_DeepInvo/blob/main/IMDN_IMDB_DeepInvo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQS9fLzPcxAi"
      },
      "outputs": [],
      "source": [
        "import os, shutil\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Go98ttvHc96n"
      },
      "source": [
        "# png2npy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "63pyko9UjvMp"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import argparse\n",
        "import skimage.io as sio\n",
        "import numpy as np\n",
        "\n",
        "parser = argparse.ArgumentParser(description='Pre-processing .png images')\n",
        "parser.add_argument('--pathFrom', default='./drive/MyDrive/dataset/DIV2K',\n",
        "                    help='directory of images to convert')\n",
        "parser.add_argument('--pathTo', default='./drive/MyDrive/dataset/DIV2K_Decoded',\n",
        "                    help='directory of images to save')\n",
        "parser.add_argument('--split', default=True,\n",
        "                    help='save individual images')\n",
        "parser.add_argument('--select', default='',\n",
        "                    help='select certain path')\n",
        "\n",
        "\n",
        "args = parser.parse_args(args = [])\n",
        "\n",
        "for (path, dirs, files) in os.walk(args.pathFrom):\n",
        "  print(path)\n",
        "  targetDir = os.path.join(args.pathTo, path[len(args.pathFrom) + 1:])\n",
        "  if len(args.select) > 0 and path.find(args.select) == -1:\n",
        "    continue\n",
        "\n",
        "  if not os.path.exists(targetDir):\n",
        "    os.mkdir(targetDir)\n",
        "\n",
        "  if len(dirs) == 0:\n",
        "    pack = {}\n",
        "    n = 0\n",
        "    for fileName in files:\n",
        "      (idx, ext) = os.path.splitext(fileName)\n",
        "      if ext == '.png':\n",
        "        image = sio.imread(os.path.join(path, fileName))\n",
        "        if args.split:\n",
        "          np.save(os.path.join(targetDir, idx + '.npy'), image)\n",
        "        n += 1\n",
        "        if n % 100 == 0:\n",
        "          print('Converted ' + str(n) + ' images.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRHlIE_5dRxO"
      },
      "source": [
        "# common"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o1zNcg7GRHfY"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import torch\n",
        "import numpy as np\n",
        "import skimage.color as sc\n",
        "\n",
        "\n",
        "def get_patch(*args, patch_size, scale):\n",
        "  ih, iw = args[0].shape[:2]\n",
        "\n",
        "  tp = patch_size  # target patch (HR)\n",
        "  ip = tp // scale  # input patch (LR)\n",
        "\n",
        "  ix = random.randrange(0, iw - ip + 1)\n",
        "  iy = random.randrange(0, ih - ip + 1)\n",
        "  tx, ty = scale * ix, scale * iy\n",
        "\n",
        "  ret = [\n",
        "      args[0][iy:iy + ip, ix:ix + ip, :],\n",
        "      *[a[ty:ty + tp, tx:tx + tp, :] for a in args[1:]]]  # results\n",
        "\n",
        "  return ret\n",
        "\n",
        "\n",
        "def set_channel(*args, n_channels=3):\n",
        "  def _set_channel(img):\n",
        "    if img.ndim == 2:\n",
        "      img = np.expand_dims(img, axis=2)\n",
        "\n",
        "    c = img.shape[2]\n",
        "    if n_channels == 1 and c == 3:\n",
        "      img = np.expand_dims(sc.rgb2ycbcr(img)[:, :, 0], 2)\n",
        "    elif n_channels == 3 and c == 1:\n",
        "      img = np.concatenate([img] * n_channels, 2)\n",
        "\n",
        "    return img\n",
        "\n",
        "  return [_set_channel(a) for a in args]\n",
        "\n",
        "\n",
        "def np2Tensor(*args, rgb_range):\n",
        "  def _np2Tensor(img):\n",
        "    np_transpose = np.ascontiguousarray(img.transpose((2, 0, 1)))\n",
        "    tensor = torch.from_numpy(np_transpose).float()\n",
        "    tensor.mul_(rgb_range / 255)\n",
        "\n",
        "    return tensor\n",
        "\n",
        "  return [_np2Tensor(a) for a in args]\n",
        "\n",
        "\n",
        "def augment(*args, hflip=True, rot=True):\n",
        "  hflip = hflip and random.random() < 0.5\n",
        "  vflip = rot and random.random() < 0.5\n",
        "  rot90 = rot and random.random() < 0.5\n",
        "\n",
        "  def _augment(img):\n",
        "    if hflip: img = img[:, ::-1, :]\n",
        "    if vflip: img = img[::-1, :, :]\n",
        "    if rot90: img = img.transpose(1, 0, 2)\n",
        "\n",
        "    return img\n",
        "\n",
        "  return [_augment(a) for a in args]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhQgvC9hddC-"
      },
      "source": [
        "# DIV2K"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FvkDNUcUQeJB"
      },
      "outputs": [],
      "source": [
        "import torch.utils.data as data\n",
        "import os.path\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def default_loader(path):\n",
        "  return cv2.imread(path, cv2.IMREAD_UNCHANGED)[:, :, [2, 1, 0]]\n",
        "\n",
        "def npy_loader(path):\n",
        "  return np.load(path)\n",
        "\n",
        "IMG_EXTENSIONS = ['.png', '.npy',\n",
        "                  ]\n",
        "\n",
        "def is_image_file(filename):\n",
        "  return any(filename.endswith(extension) for extension in IMG_EXTENSIONS)\n",
        "\n",
        "def make_dataset(dir):\n",
        "  images = []\n",
        "  assert os.path.isdir(dir), '%s is not a valid directory' % dir\n",
        "\n",
        "  for root, _, fnames in sorted(os.walk(dir)):\n",
        "    for fname in fnames:\n",
        "      if is_image_file(fname):\n",
        "        path = os.path.join(root, fname)\n",
        "        images.append(path)\n",
        "  return images\n",
        "\n",
        "\n",
        "class div2k(data.Dataset):\n",
        "  def __init__(self, opt):\n",
        "    self.opt = opt\n",
        "    self.scale = self.opt.scale\n",
        "    self.root = self.opt.root\n",
        "    self.ext = self.opt.ext   # '.png' or '.npy'(default)\n",
        "    self.train = True if self.opt.phase == 'train' else False\n",
        "    self.repeat =10 #self.opt.test_every // (self.opt.n_train // self.opt.batch_size)\n",
        "    self._set_filesystem(self.root)\n",
        "    self.images_hr, self.images_lr = self._scan()\n",
        "\n",
        "  def _set_filesystem(self, dir_data):\n",
        "    self.root = dir_data + '/DIV2K_Decoded'\n",
        "    self.dir_hr = os.path.join(self.root, 'DIV2K_train_HR')\n",
        "    self.dir_lr = os.path.join(self.root, 'DIV2K_train_LR_bicubic/X' + str(self.scale))\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    lr, hr = self._load_file(idx)\n",
        "    lr, hr = self._get_patch(lr, hr)\n",
        "    lr, hr = set_channel(lr, hr, n_channels=self.opt.n_colors)\n",
        "    lr_tensor, hr_tensor = np2Tensor(lr, hr, rgb_range=self.opt.rgb_range)\n",
        "    return lr_tensor, hr_tensor\n",
        "\n",
        "  def __len__(self):\n",
        "    if self.train:\n",
        "      return self.opt.n_train * self.repeat\n",
        "\n",
        "  def _get_index(self, idx):\n",
        "    if self.train:\n",
        "      return idx % self.opt.n_train\n",
        "    else:\n",
        "      return idx\n",
        "\n",
        "  def _get_patch(self, img_in, img_tar):\n",
        "    patch_size = self.opt.patch_size\n",
        "    scale = self.scale\n",
        "    if self.train:\n",
        "      img_in, img_tar = get_patch(img_in, img_tar, patch_size=patch_size, scale=scale)\n",
        "      img_in, img_tar = augment(img_in, img_tar)\n",
        "    else:\n",
        "      ih, iw = img_in.shape[:2]\n",
        "      img_tar = img_tar[0:ih * scale, 0:iw * scale, :]\n",
        "    return img_in, img_tar\n",
        "\n",
        "  def _scan(self):\n",
        "    list_hr = sorted(make_dataset(self.dir_hr))\n",
        "    list_lr = sorted(make_dataset(self.dir_lr))\n",
        "    return list_hr, list_lr\n",
        "\n",
        "  def _load_file(self, idx):\n",
        "    idx = self._get_index(idx)\n",
        "    if self.ext == '.npy':\n",
        "      lr = npy_loader(self.images_lr[idx])\n",
        "      hr = npy_loader(self.images_hr[idx])\n",
        "    else:\n",
        "      lr = default_loader(self.images_lr[idx])\n",
        "      hr = default_loader(self.images_hr[idx])\n",
        "    return lr, hr"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set5"
      ],
      "metadata": {
        "id": "GgxD35D5dyRP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8KbRwKvjVI8t"
      },
      "outputs": [],
      "source": [
        "import torch.utils.data as data\n",
        "from os.path import join\n",
        "from os import listdir\n",
        "\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from torchvision.transforms import Compose\n",
        "\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "def img_modcrop(image, modulo):\n",
        "    sz = image.size\n",
        "    w = np.int32(sz[0] / modulo) * modulo\n",
        "    h = np.int32(sz[1] / modulo) * modulo\n",
        "    out = image.crop((0, 0, w, h))\n",
        "    return out\n",
        "\n",
        "\n",
        "def np2tensor():\n",
        "    return Compose([\n",
        "        ToTensor(),\n",
        "    ])\n",
        "\n",
        "\n",
        "def is_image_file(filename):\n",
        "      return any(filename.endswith(extension) for extension in [\".bmp\", \".png\", \".jpg\"])\n",
        "\n",
        "\n",
        "def load_image(filepath):\n",
        "    return Image.open(filepath).convert('RGB')\n",
        "\n",
        "\n",
        "class DatasetFromFolderVal(data.Dataset):\n",
        "    def __init__(self, hr_dir, lr_dir, upscale):\n",
        "        super(DatasetFromFolderVal, self).__init__()\n",
        "        self.hr_filenames = sorted([join(hr_dir, x) for x in listdir(hr_dir) if is_image_file(x)])\n",
        "        self.lr_filenames = sorted([join(lr_dir, x) for x in listdir(lr_dir) if is_image_file(x)])\n",
        "        self.upscale = upscale\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        input = load_image(self.lr_filenames[index])\n",
        "        target = load_image(self.hr_filenames[index])\n",
        "        input = np2tensor()(input)\n",
        "        target = np2tensor()(img_modcrop(target, self.upscale))\n",
        "\n",
        "        return input, target\n",
        "\n",
        "    def __len__(self):\n",
        "          return len(self.lr_filenames)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jg72UmaSdm69"
      },
      "source": [
        "# utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89MV6rKlSKL2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "from collections import OrderedDict\n",
        "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "\n",
        "def compute_psnr(im1, im2):\n",
        "  p = psnr(im1, im2)\n",
        "  return p\n",
        "\n",
        "\n",
        "def compute_ssim(im1, im2):\n",
        "  isRGB = len(im1.shape) == 3 and im1.shape[-1] == 3\n",
        "  s = ssim(im1, im2, K1=0.01, K2=0.03, gaussian_weights=True, sigma=1.5, use_sample_covariance=False,\n",
        "           multichannel=isRGB)\n",
        "  return s\n",
        "\n",
        "\n",
        "def shave(im, border):\n",
        "  border = [border, border]\n",
        "  im = im[border[0]:-border[0], border[1]:-border[1], ...]\n",
        "  return im\n",
        "\n",
        "\n",
        "def modcrop(im, modulo):\n",
        "  sz = im.shape\n",
        "  h = np.int32(sz[0] / modulo) * modulo\n",
        "  w = np.int32(sz[1] / modulo) * modulo\n",
        "  ims = im[0:h, 0:w, ...]\n",
        "  return ims\n",
        "\n",
        "\n",
        "def get_list(path, ext):\n",
        "  return [os.path.join(path, f) for f in os.listdir(path) if f.endswith(ext)]\n",
        "\n",
        "\n",
        "def convert_shape(img):\n",
        "  img = np.transpose((img * 255.0).round(), (1, 2, 0))\n",
        "  img = np.uint8(np.clip(img, 0, 255))\n",
        "  return img\n",
        "\n",
        "\n",
        "def quantize(img):\n",
        "  return img.clip(0, 255).round().astype(np.uint8)\n",
        "\n",
        "\n",
        "def tensor2np(tensor, out_type=np.uint8, min_max=(0, 1)):\n",
        "  tensor = tensor.float().cpu().clamp_(*min_max)\n",
        "  tensor = (tensor - min_max[0]) / (min_max[1] - min_max[0])  # to range [0, 1]\n",
        "  img_np = tensor.numpy()\n",
        "  img_np = np.transpose(img_np, (1, 2, 0))\n",
        "  if out_type == np.uint8:\n",
        "    img_np = (img_np * 255.0).round()\n",
        "\n",
        "  return img_np.astype(out_type)\n",
        "\n",
        "def convert2np(tensor):\n",
        "  return tensor.cpu().mul(255).clamp(0, 255).byte().squeeze().permute(1, 2, 0).numpy()\n",
        "\n",
        "\n",
        "def adjust_learning_rate(optimizer, epoch, step_size, lr_init, gamma):\n",
        "  factor = epoch // step_size\n",
        "  lr = lr_init * (gamma ** factor)\n",
        "  for param_group in optimizer.param_groups:\n",
        "    param_group['lr'] = lr\n",
        "\n",
        "def load_state_dict(path):\n",
        "\n",
        "  state_dict = torch.load(path)\n",
        "  new_state_dcit = OrderedDict()\n",
        "  for k, v in state_dict.items():\n",
        "    if 'module' in k:\n",
        "      name = k[7:]\n",
        "    else:\n",
        "      name = k\n",
        "      new_state_dcit[name] = v\n",
        "  return new_state_dcit\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSWXt-7GRKj3"
      },
      "source": [
        "# Involution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5tAF8Bz7Re17"
      },
      "outputs": [],
      "source": [
        "!pip install mmcv==2.0.0 -f https://download.openmmlab.com/mmcv/dist/cu118/torch2.0/index.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w3rjlQNckTL0"
      },
      "outputs": [],
      "source": [
        "from torch.autograd import Function\n",
        "import torch\n",
        "from torch.nn.modules.utils import _pair\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from mmcv.cnn import ConvModule\n",
        "\n",
        "\n",
        "from collections import namedtuple\n",
        "\n",
        "import cupy\n",
        "from string import Template\n",
        "\n",
        "\n",
        "Stream = namedtuple('Stream', ['ptr'])\n",
        "\n",
        "\n",
        "def Dtype(t):\n",
        "    if isinstance(t, torch.cuda.FloatTensor):\n",
        "        return 'float'\n",
        "    elif isinstance(t, torch.cuda.DoubleTensor):\n",
        "        return 'double'\n",
        "\n",
        "\n",
        "@cupy._util.memoize(for_each_device=True)\n",
        "def load_kernel(kernel_name, code, **kwargs):\n",
        "    code = Template(code).substitute(**kwargs)\n",
        "    kernel_code = cupy.cuda.compile_with_cache(code)\n",
        "    return kernel_code.get_function(kernel_name)\n",
        "\n",
        "\n",
        "CUDA_NUM_THREADS = 1024\n",
        "\n",
        "kernel_loop = '''\n",
        "#define CUDA_KERNEL_LOOP(i, n)                        \\\n",
        "  for (int i = blockIdx.x * blockDim.x + threadIdx.x; \\\n",
        "      i < (n);                                       \\\n",
        "      i += blockDim.x * gridDim.x)\n",
        "'''\n",
        "\n",
        "\n",
        "def GET_BLOCKS(N):\n",
        "    return (N + CUDA_NUM_THREADS - 1) // CUDA_NUM_THREADS\n",
        "\n",
        "\n",
        "_involution_kernel = kernel_loop + '''\n",
        "extern \"C\"\n",
        "__global__ void involution_forward_kernel(\n",
        "const ${Dtype}* bottom_data, const ${Dtype}* weight_data, ${Dtype}* top_data) {\n",
        "  CUDA_KERNEL_LOOP(index, ${nthreads}) {\n",
        "    const int n = index / ${channels} / ${top_height} / ${top_width};\n",
        "    const int c = (index / ${top_height} / ${top_width}) % ${channels};\n",
        "    const int h = (index / ${top_width}) % ${top_height};\n",
        "    const int w = index % ${top_width};\n",
        "    const int g = c / (${channels} / ${groups});\n",
        "    ${Dtype} value = 0;\n",
        "    #pragma unroll\n",
        "    for (int kh = 0; kh < ${kernel_h}; ++kh) {\n",
        "      #pragma unroll\n",
        "      for (int kw = 0; kw < ${kernel_w}; ++kw) {\n",
        "        const int h_in = -${pad_h} + h * ${stride_h} + kh * ${dilation_h};\n",
        "        const int w_in = -${pad_w} + w * ${stride_w} + kw * ${dilation_w};\n",
        "        if ((h_in >= 0) && (h_in < ${bottom_height})\n",
        "          && (w_in >= 0) && (w_in < ${bottom_width})) {\n",
        "          const int offset = ((n * ${channels} + c) * ${bottom_height} + h_in)\n",
        "            * ${bottom_width} + w_in;\n",
        "          const int offset_weight = ((((n * ${groups} + g) * ${kernel_h} + kh) * ${kernel_w} + kw) * ${top_height} + h)\n",
        "            * ${top_width} + w;\n",
        "          value += weight_data[offset_weight] * bottom_data[offset];\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "    top_data[index] = value;\n",
        "  }\n",
        "}\n",
        "'''\n",
        "\n",
        "\n",
        "_involution_kernel_backward_grad_input = kernel_loop + '''\n",
        "extern \"C\"\n",
        "__global__ void involution_backward_grad_input_kernel(\n",
        "    const ${Dtype}* const top_diff, const ${Dtype}* const weight_data, ${Dtype}* const bottom_diff) {\n",
        "  CUDA_KERNEL_LOOP(index, ${nthreads}) {\n",
        "    const int n = index / ${channels} / ${bottom_height} / ${bottom_width};\n",
        "    const int c = (index / ${bottom_height} / ${bottom_width}) % ${channels};\n",
        "    const int h = (index / ${bottom_width}) % ${bottom_height};\n",
        "    const int w = index % ${bottom_width};\n",
        "    const int g = c / (${channels} / ${groups});\n",
        "    ${Dtype} value = 0;\n",
        "    #pragma unroll\n",
        "    for (int kh = 0; kh < ${kernel_h}; ++kh) {\n",
        "      #pragma unroll\n",
        "      for (int kw = 0; kw < ${kernel_w}; ++kw) {\n",
        "        const int h_out_s = h + ${pad_h} - kh * ${dilation_h};\n",
        "        const int w_out_s = w + ${pad_w} - kw * ${dilation_w};\n",
        "        if (((h_out_s % ${stride_h}) == 0) && ((w_out_s % ${stride_w}) == 0)) {\n",
        "          const int h_out = h_out_s / ${stride_h};\n",
        "          const int w_out = w_out_s / ${stride_w};\n",
        "          if ((h_out >= 0) && (h_out < ${top_height})\n",
        "                && (w_out >= 0) && (w_out < ${top_width})) {\n",
        "            const int offset = ((n * ${channels} + c) * ${top_height} + h_out)\n",
        "                  * ${top_width} + w_out;\n",
        "            const int offset_weight = ((((n * ${groups} + g) * ${kernel_h} + kh) * ${kernel_w} + kw) * ${top_height} + h_out)\n",
        "                  * ${top_width} + w_out;\n",
        "            value += weight_data[offset_weight] * top_diff[offset];\n",
        "          }\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "    bottom_diff[index] = value;\n",
        "  }\n",
        "}\n",
        "'''\n",
        "\n",
        "\n",
        "_involution_kernel_backward_grad_weight = kernel_loop + '''\n",
        "extern \"C\"\n",
        "__global__ void involution_backward_grad_weight_kernel(\n",
        "    const ${Dtype}* const top_diff, const ${Dtype}* const bottom_data, ${Dtype}* const buffer_data) {\n",
        "  CUDA_KERNEL_LOOP(index, ${nthreads}) {\n",
        "    const int h = (index / ${top_width}) % ${top_height};\n",
        "    const int w = index % ${top_width};\n",
        "    const int kh = (index / ${kernel_w} / ${top_height} / ${top_width})\n",
        "          % ${kernel_h};\n",
        "    const int kw = (index / ${top_height} / ${top_width}) % ${kernel_w};\n",
        "    const int h_in = -${pad_h} + h * ${stride_h} + kh * ${dilation_h};\n",
        "    const int w_in = -${pad_w} + w * ${stride_w} + kw * ${dilation_w};\n",
        "    if ((h_in >= 0) && (h_in < ${bottom_height})\n",
        "          && (w_in >= 0) && (w_in < ${bottom_width})) {\n",
        "      const int g = (index / ${kernel_h} / ${kernel_w} / ${top_height} / ${top_width}) % ${groups};\n",
        "      const int n = (index / ${groups} / ${kernel_h} / ${kernel_w} / ${top_height} / ${top_width}) % ${num};\n",
        "      ${Dtype} value = 0;\n",
        "      #pragma unroll\n",
        "      for (int c = g * (${channels} / ${groups}); c < (g + 1) * (${channels} / ${groups}); ++c) {\n",
        "        const int top_offset = ((n * ${channels} + c) * ${top_height} + h)\n",
        "              * ${top_width} + w;\n",
        "        const int bottom_offset = ((n * ${channels} + c) * ${bottom_height} + h_in)\n",
        "              * ${bottom_width} + w_in;\n",
        "        value += top_diff[top_offset] * bottom_data[bottom_offset];\n",
        "      }\n",
        "      buffer_data[index] = value;\n",
        "    } else {\n",
        "      buffer_data[index] = 0;\n",
        "    }\n",
        "  }\n",
        "}\n",
        "'''\n",
        "\n",
        "\n",
        "class _involution(Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input, weight, stride, padding, dilation):\n",
        "        assert input.dim() == 4 and input.is_cuda\n",
        "        assert weight.dim() == 6 and weight.is_cuda\n",
        "        batch_size, channels, height, width = input.size()\n",
        "        kernel_h, kernel_w = weight.size()[2:4]\n",
        "        output_h = int((height + 2 * padding[0] - (dilation[0] * (kernel_h - 1) + 1)) / stride[0] + 1)\n",
        "        output_w = int((width + 2 * padding[1] - (dilation[1] * (kernel_w - 1) + 1)) / stride[1] + 1)\n",
        "\n",
        "        output = input.new(batch_size, channels, output_h, output_w)\n",
        "        n = output.numel()\n",
        "\n",
        "        with torch.cuda.device_of(input):\n",
        "            f = load_kernel('involution_forward_kernel', _involution_kernel, Dtype=Dtype(input), nthreads=n,\n",
        "                            num=batch_size, channels=channels, groups=weight.size()[1],\n",
        "                            bottom_height=height, bottom_width=width,\n",
        "                            top_height=output_h, top_width=output_w,\n",
        "                            kernel_h=kernel_h, kernel_w=kernel_w,\n",
        "                            stride_h=stride[0], stride_w=stride[1],\n",
        "                            dilation_h=dilation[0], dilation_w=dilation[1],\n",
        "                            pad_h=padding[0], pad_w=padding[1])\n",
        "            f(block=(CUDA_NUM_THREADS,1,1),\n",
        "              grid=(GET_BLOCKS(n),1,1),\n",
        "              args=[input.data_ptr(), weight.data_ptr(), output.data_ptr()],\n",
        "              stream=Stream(ptr=torch.cuda.current_stream().cuda_stream))\n",
        "\n",
        "        ctx.save_for_backward(input, weight)\n",
        "        ctx.stride, ctx.padding, ctx.dilation = stride, padding, dilation\n",
        "        return output\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        assert grad_output.is_cuda and grad_output.is_contiguous()\n",
        "        input, weight = ctx.saved_tensors\n",
        "        stride, padding, dilation = ctx.stride, ctx.padding, ctx.dilation\n",
        "\n",
        "        batch_size, channels, height, width = input.size()\n",
        "        kernel_h, kernel_w = weight.size()[2:4]\n",
        "        output_h, output_w = grad_output.size()[2:]\n",
        "\n",
        "        grad_input, grad_weight = None, None\n",
        "\n",
        "        opt = dict(Dtype=Dtype(grad_output),\n",
        "                   num=batch_size, channels=channels, groups=weight.size()[1],\n",
        "                   bottom_height=height, bottom_width=width,\n",
        "                   top_height=output_h, top_width=output_w,\n",
        "                   kernel_h=kernel_h, kernel_w=kernel_w,\n",
        "                   stride_h=stride[0], stride_w=stride[1],\n",
        "                   dilation_h=dilation[0], dilation_w=dilation[1],\n",
        "                   pad_h=padding[0], pad_w=padding[1])\n",
        "\n",
        "        with torch.cuda.device_of(input):\n",
        "            if ctx.needs_input_grad[0]:\n",
        "                grad_input = input.new(input.size())\n",
        "\n",
        "                n = grad_input.numel()\n",
        "                opt['nthreads'] = n\n",
        "\n",
        "                f = load_kernel('involution_backward_grad_input_kernel',\n",
        "                                _involution_kernel_backward_grad_input, **opt)\n",
        "                f(block=(CUDA_NUM_THREADS,1,1),\n",
        "                  grid=(GET_BLOCKS(n),1,1),\n",
        "                  args=[grad_output.data_ptr(), weight.data_ptr(), grad_input.data_ptr()],\n",
        "                  stream=Stream(ptr=torch.cuda.current_stream().cuda_stream))\n",
        "\n",
        "            if ctx.needs_input_grad[1]:\n",
        "                grad_weight = weight.new(weight.size())\n",
        "\n",
        "                n = grad_weight.numel()\n",
        "                opt['nthreads'] = n\n",
        "\n",
        "                f = load_kernel('involution_backward_grad_weight_kernel',\n",
        "                                _involution_kernel_backward_grad_weight, **opt)\n",
        "                f(block=(CUDA_NUM_THREADS,1,1),\n",
        "                  grid=(GET_BLOCKS(n),1,1),\n",
        "                  args=[grad_output.data_ptr(), input.data_ptr(), grad_weight.data_ptr()],\n",
        "                  stream=Stream(ptr=torch.cuda.current_stream().cuda_stream))\n",
        "\n",
        "        return grad_input, grad_weight, None, None, None\n",
        "\n",
        "\n",
        "def _involution_cuda(input, weight, bias=None, stride=1, padding=0, dilation=1):\n",
        "    \"\"\" involution kernel\n",
        "    \"\"\"\n",
        "    assert input.size(0) == weight.size(0)\n",
        "    assert input.size(-2)//stride == weight.size(-2)\n",
        "    assert input.size(-1)//stride == weight.size(-1)\n",
        "    if input.is_cuda:\n",
        "        out = _involution.apply(input, weight, _pair(stride), _pair(padding), _pair(dilation))\n",
        "        if bias is not None:\n",
        "            out += bias.view(1,-1,1,1)\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "    return out\n",
        "\n",
        "\n",
        "class involution(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 channels,\n",
        "                 kernel_size,\n",
        "                 stride):\n",
        "        super(involution, self).__init__()\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "        self.channels = channels\n",
        "        reduction_ratio = 4\n",
        "        self.group_channels = 16\n",
        "        self.groups = self.channels // self.group_channels\n",
        "        self.conv1 = ConvModule(\n",
        "            in_channels=channels,\n",
        "            out_channels=channels // reduction_ratio,\n",
        "            kernel_size=1,\n",
        "            conv_cfg=None,\n",
        "            norm_cfg=dict(type='BN'),\n",
        "            act_cfg=dict(type='ReLU'))\n",
        "        self.conv2 = ConvModule(\n",
        "            in_channels=channels // reduction_ratio,\n",
        "            out_channels=kernel_size**2 * self.groups,\n",
        "            kernel_size=1,\n",
        "            stride=1,\n",
        "            conv_cfg=None,\n",
        "            norm_cfg=None,\n",
        "            act_cfg=None)\n",
        "        if stride > 1:\n",
        "            self.avgpool = nn.AvgPool2d(stride, stride)\n",
        "\n",
        "    def forward(self, x):\n",
        "        weight = self.conv2(self.conv1(x if self.stride == 1 else self.avgpool(x)))\n",
        "        b, c, h, w = weight.shape\n",
        "        weight = weight.view(b, self.groups, self.kernel_size, self.kernel_size, h, w)\n",
        "        out = _involution_cuda(x, weight, stride=self.stride, padding=(self.kernel_size-1)//2)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEcmFpUXdt1f"
      },
      "source": [
        "# block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iUwLHdib2ONY"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "from collections import OrderedDict\n",
        "import torch\n",
        "\n",
        "\n",
        "def conv_layer(in_channels, out_channels, kernel_size, stride=1, dilation=1, groups=1, bias=True):\n",
        "  padding = int((kernel_size - 1) / 2) * dilation\n",
        "  return nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding=padding, bias=bias, dilation=dilation,\n",
        "                   groups=groups)\n",
        "\n",
        "\n",
        "def norm(norm_type, nc):\n",
        "  norm_type = norm_type.lower()\n",
        "  if norm_type == 'batch':\n",
        "    layer = nn.BatchNorm2d(nc, affine=True)\n",
        "  elif norm_type == 'instance':\n",
        "    layer = nn.InstanceNorm2d(nc, affine=False)\n",
        "  else:\n",
        "    raise NotImplementedError('normalization layer [{:s}] is not found'.format(norm_type))\n",
        "  return layer\n",
        "\n",
        "\n",
        "def pad(pad_type, padding):\n",
        "  pad_type = pad_type.lower()\n",
        "  if padding == 0:\n",
        "    return None\n",
        "  if pad_type == 'reflect':\n",
        "    layer = nn.ReflectionPad2d(padding)\n",
        "  elif pad_type == 'replicate':\n",
        "    layer = nn.ReplicationPad2d(padding)\n",
        "  else:\n",
        "    raise NotImplementedError('padding layer [{:s}] is not implemented'.format(pad_type))\n",
        "  return layer\n",
        "\n",
        "\n",
        "def get_valid_padding(kernel_size, dilation):\n",
        "  kernel_size = kernel_size + (kernel_size - 1) * (dilation - 1)\n",
        "  padding = (kernel_size - 1) // 2\n",
        "  return padding\n",
        "\n",
        "\n",
        "def conv_block(in_nc, out_nc, kernel_size, stride=1, dilation=1, groups=1, bias=True,\n",
        "               pad_type='zero', norm_type=None, act_type='relu'):\n",
        "  padding = get_valid_padding(kernel_size, dilation)\n",
        "  p = pad(pad_type, padding) if pad_type and pad_type != 'zero' else None\n",
        "  padding = padding if pad_type == 'zero' else 0\n",
        "\n",
        "  c = nn.Conv2d(in_nc, out_nc, kernel_size=kernel_size, stride=stride, padding=padding,\n",
        "                  dilation=dilation, bias=bias, groups=groups)\n",
        "  a = activation(act_type) if act_type else None\n",
        "  n = norm(norm_type, out_nc) if norm_type else None\n",
        "  return sequential(p, c, n, a)\n",
        "\n",
        "\n",
        "def activation(act_type, inplace=True, neg_slope=0.05, n_prelu=1):\n",
        "  act_type = act_type.lower()\n",
        "  if act_type == 'relu':\n",
        "    layer = nn.ReLU(inplace)\n",
        "  elif act_type == 'lrelu':\n",
        "    layer = nn.LeakyReLU(neg_slope, inplace)\n",
        "  elif act_type == 'prelu':\n",
        "    layer = nn.PReLU(num_parameters=n_prelu, init=neg_slope)\n",
        "  else:\n",
        "    raise NotImplementedError('activation layer [{:s}] is not found'.format(act_type))\n",
        "  return layer\n",
        "\n",
        "\n",
        "class ShortcutBlock(nn.Module):\n",
        "  def __init__(self, submodule):\n",
        "    super(ShortcutBlock, self).__init__()\n",
        "    self.sub = submodule\n",
        "\n",
        "  def forward(self, x):\n",
        "    output = x + self.sub(x)\n",
        "    return output\n",
        "\n",
        "\n",
        "\n",
        "def mean_channels(F):\n",
        "  assert(F.dim() == 4)\n",
        "  spatial_sum = F.sum(3, keepdim=True).sum(2, keepdim=True)\n",
        "  return spatial_sum / (F.size(2) * F.size(3))\n",
        "\n",
        "def stdv_channels(F):\n",
        "  assert(F.dim() == 4)\n",
        "  F_mean = mean_channels(F)\n",
        "  F_variance = (F - F_mean).pow(2).sum(3, keepdim=True).sum(2, keepdim=True) / (F.size(2) * F.size(3))\n",
        "  return F_variance.pow(0.5)\n",
        "\n",
        "def sequential(*args):\n",
        "  if len(args) == 1:\n",
        "    if isinstance(args[0], OrderedDict):\n",
        "      raise NotImplementedError('sequential does not support OrderedDict input.')\n",
        "    return args[0]\n",
        "  modules = []\n",
        "  for module in args:\n",
        "    if isinstance(module, nn.Sequential):\n",
        "      for submodule in module.children():\n",
        "        modules.append(submodule)\n",
        "    elif isinstance(module, nn.Module):\n",
        "      modules.append(module)\n",
        "  return nn.Sequential(*modules)\n",
        "\n",
        "\n",
        "\n",
        "# contrast-aware channel attention module\n",
        "class CCALayer(nn.Module):\n",
        "  def __init__(self, channel, reduction=16):\n",
        "    super(CCALayer, self).__init__()\n",
        "\n",
        "    self.contrast = stdv_channels\n",
        "    self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "    self.conv_du = nn.Sequential(\n",
        "        nn.Conv2d(channel, channel // reduction, 1, padding=0, bias=True),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Conv2d(channel // reduction, channel, 1, padding=0, bias=True),\n",
        "        nn.Sigmoid()\n",
        "      )\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    y = self.contrast(x) + self.avg_pool(x)\n",
        "    y = self.conv_du(y)\n",
        "    return x * y\n",
        "\n",
        "\n",
        "class IMDModule1(nn.Module):\n",
        "  def __init__(self, in_channels, distillation_rate=0.25):\n",
        "    super(IMDModule1, self).__init__()\n",
        "    self.distilled_channels = int(in_channels * distillation_rate)\n",
        "    self.remaining_channels = int(in_channels - self.distilled_channels)\n",
        "    self.c1 = conv_layer(in_channels, in_channels, 3)\n",
        "    self.c2 = conv_layer(self.remaining_channels, in_channels, 3)\n",
        "    self.c3 = conv_layer(self.remaining_channels, in_channels, 3)\n",
        "    self.c4 = conv_layer(self.remaining_channels, self.distilled_channels, 3)\n",
        "    self.act = activation('lrelu', neg_slope=0.05)\n",
        "    self.c5 = conv_layer(in_channels, in_channels, 1)\n",
        "    self.cca = CCALayer(self.distilled_channels * 4)\n",
        "\n",
        "  def forward(self, input):\n",
        "    out_c1 = self.act(self.c1(input))\n",
        "    distilled_c1, remaining_c1 = torch.split(out_c1, (self.distilled_channels, self.remaining_channels), dim=1)\n",
        "    out_c2 = self.act(self.c2(remaining_c1))\n",
        "    distilled_c2, remaining_c2 = torch.split(out_c2, (self.distilled_channels, self.remaining_channels), dim=1)\n",
        "    out_c3 = self.act(self.c3(remaining_c2))\n",
        "    distilled_c3, remaining_c3 = torch.split(out_c3, (self.distilled_channels, self.remaining_channels), dim=1)\n",
        "    out_c4 = self.c4(remaining_c3)\n",
        "    out = torch.cat([distilled_c1, distilled_c2, distilled_c3, out_c4], dim=1)\n",
        "    out_fused = self.c5(self.cca(out)) + input\n",
        "    return out_fused\n",
        "\n",
        "\n",
        "class IMDModule2(nn.Module):\n",
        "  def __init__(self, in_channels, distillation_rate=0.25):\n",
        "    super(IMDModule2, self).__init__()\n",
        "    self.distilled_channels = int(in_channels * distillation_rate)\n",
        "    self.remaining_channels = int(in_channels - self.distilled_channels)\n",
        "    self.c1 = conv_layer(in_channels, in_channels, 3)\n",
        "    self.c2 = conv_layer(self.remaining_channels, in_channels, 3)\n",
        "    self.c3 = involution(in_channels , 3 , 1)#input channels,kernel_size, stride\n",
        "    self.c4 = conv_layer(self.remaining_channels, self.distilled_channels, 3)\n",
        "    self.act = activation('lrelu', neg_slope=0.05)\n",
        "    self.c5 = conv_layer(in_channels, in_channels, 1)\n",
        "    self.cca = CCALayer(self.distilled_channels * 4)\n",
        "\n",
        "  def forward(self, input):\n",
        "    out_c1 = self.act(self.c1(input))\n",
        "    distilled_c1, remaining_c1 = torch.split(out_c1, (self.distilled_channels, self.remaining_channels), dim=1)\n",
        "    out_c2 = self.act(self.c2(remaining_c1))\n",
        "    distilled_c2, remaining_c2 = torch.split(out_c2, (self.distilled_channels, self.remaining_channels), dim=1)\n",
        "    out_c3 = self.act(self.c3(out_c2))\n",
        "    distilled_c3, remaining_c3 = torch.split(out_c3, (self.distilled_channels, self.remaining_channels), dim=1)\n",
        "    out_c4 = self.c4(remaining_c3)\n",
        "    out = torch.cat([distilled_c1, distilled_c2, distilled_c3, out_c4], dim=1)\n",
        "    out_fused = self.c5(self.cca(out)) + input\n",
        "    return out_fused\n",
        "\n",
        "\n",
        "class IMDModule3(nn.Module):\n",
        "  def __init__(self, in_channels, distillation_rate=0.25):\n",
        "    super(IMDModule3, self).__init__()\n",
        "    self.distilled_channels = int(in_channels * distillation_rate)\n",
        "    self.remaining_channels = int(in_channels - self.distilled_channels)\n",
        "    self.c1 = involution(in_channels , 3 , 1)#input channels,kernel_size, stride\n",
        "    self.c2 = conv_layer(self.remaining_channels, in_channels, 3)\n",
        "    self.c3 = involution(in_channels , 3 , 1)\n",
        "    self.c4 = conv_layer(self.remaining_channels, self.distilled_channels, 3)\n",
        "    self.act = activation('lrelu', neg_slope=0.05)\n",
        "    self.c5 = conv_layer(in_channels, in_channels, 1)\n",
        "    self.cca = CCALayer(self.distilled_channels * 4)\n",
        "\n",
        "  def forward(self, input):\n",
        "    out_c1 = self.act(self.c1(input))\n",
        "    distilled_c1, remaining_c1 = torch.split(out_c1, (self.distilled_channels, self.remaining_channels), dim=1)\n",
        "    out_c2 = self.act(self.c2(remaining_c1))\n",
        "    distilled_c2, remaining_c2 = torch.split(out_c2, (self.distilled_channels, self.remaining_channels), dim=1)\n",
        "    out_c3 = self.act(self.c3(out_c2))\n",
        "    distilled_c3, remaining_c3 = torch.split(out_c3, (self.distilled_channels, self.remaining_channels), dim=1)\n",
        "    out_c4 = self.c4(remaining_c3)\n",
        "    out = torch.cat([distilled_c1, distilled_c2, distilled_c3, out_c4], dim=1)\n",
        "    out_fused = self.c5(self.cca(out)) + input\n",
        "    return out_fused\n",
        "\n",
        "\n",
        "class IMDModule4(nn.Module):\n",
        "  def __init__(self, in_channels, distillation_rate=0.25):\n",
        "    super(IMDModule4, self).__init__()\n",
        "    self.distilled_channels = int(in_channels * distillation_rate)\n",
        "    self.remaining_channels = int(in_channels - self.distilled_channels)\n",
        "    self.c1 = involution(in_channels , 3 , 1)\n",
        "    self.c2 = involution(in_channels , 3 , 1)#input channels,kernel_size, stride\n",
        "    self.c3 = involution(in_channels , 3 , 1)\n",
        "    self.c4 = conv_layer(self.remaining_channels, self.distilled_channels, 3)\n",
        "    self.act = activation('lrelu', neg_slope=0.05)\n",
        "    self.c5 = conv_layer(in_channels, in_channels, 1)\n",
        "    self.cca = CCALayer(self.distilled_channels * 4)\n",
        "\n",
        "  def forward(self, input):\n",
        "    out_c1 = self.act(self.c1(input))\n",
        "    distilled_c1, remaining_c1 = torch.split(out_c1, (self.distilled_channels, self.remaining_channels), dim=1)\n",
        "    out_c2 = self.act(self.c2(out_c1))\n",
        "    distilled_c2, remaining_c2 = torch.split(out_c2, (self.distilled_channels, self.remaining_channels), dim=1)\n",
        "    out_c3 = self.act(self.c3(out_c2))\n",
        "    distilled_c3, remaining_c3 = torch.split(out_c3, (self.distilled_channels, self.remaining_channels), dim=1)\n",
        "    out_c4 = self.c4(remaining_c3)\n",
        "    out = torch.cat([distilled_c1, distilled_c2, distilled_c3, out_c4], dim=1)\n",
        "    out_fused = self.c5(self.cca(out)) + input\n",
        "    return out_fused\n",
        "\n",
        "def pixelshuffle_block(in_channels, out_channels, upscale_factor=2, kernel_size=3, stride=1):\n",
        "  conv = conv_layer(in_channels, out_channels * (upscale_factor ** 2), kernel_size, stride)\n",
        "  pixel_shuffle = nn.PixelShuffle(upscale_factor)\n",
        "  return sequential(conv, pixel_shuffle)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VlqdWdl-d1qG"
      },
      "source": [
        "#  model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6YRUBk-kSKId"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "\n",
        "\n",
        "class IMDN(nn.Module):\n",
        "  def __init__(self, in_nc=3, nf=64, num_modules=6, out_nc=3, upscale=4):\n",
        "    super(IMDN, self).__init__()\n",
        "\n",
        "    self.fea_conv = conv_layer(in_nc, nf, kernel_size=3)\n",
        "\n",
        "    # IMDBs\n",
        "    self.IMDB1 = IMDModule1(in_channels=nf)\n",
        "    self.IMDB2 = IMDModule1(in_channels=nf)\n",
        "    self.IMDB3 = IMDModule2(in_channels=nf)\n",
        "    self.IMDB4 = IMDModule2(in_channels=nf)\n",
        "    self.IMDB5 = IMDModule3(in_channels=nf)\n",
        "    self.IMDB6 = IMDModule4(in_channels=nf)\n",
        "    self.c = conv_block(nf * num_modules, nf, kernel_size=1, act_type='lrelu')\n",
        "\n",
        "    self.LR_conv = conv_layer(nf, nf, kernel_size=3)\n",
        "\n",
        "    upsample_block = pixelshuffle_block\n",
        "    self.upsampler = upsample_block(nf, out_nc, upscale_factor=upscale)\n",
        "\n",
        "\n",
        "  def forward(self, input):\n",
        "    out_fea = self.fea_conv(input)\n",
        "    out_B1 = self.IMDB1(out_fea)\n",
        "    out_B2 = self.IMDB2(out_B1)\n",
        "    out_B3 = self.IMDB3(out_B2)\n",
        "    out_B4 = self.IMDB4(out_B3)\n",
        "    out_B5 = self.IMDB5(out_B4)\n",
        "    out_B6 = self.IMDB6(out_B5)\n",
        "\n",
        "    out_B = self.c(torch.cat([out_B1, out_B2, out_B3, out_B4, out_B5, out_B6], dim=1))\n",
        "    out_lr = self.LR_conv(out_B) + out_fea\n",
        "    output = self.upsampler(out_lr)\n",
        "    return output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xay-Lf3LYDHI"
      },
      "source": [
        "# loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1QghcqDEYEyo"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "\n",
        "class VGGPerceptualLoss(torch.nn.Module):\n",
        "    def __init__(self, resize=True):\n",
        "        super(VGGPerceptualLoss, self).__init__()\n",
        "        blocks = []\n",
        "        blocks.append(torchvision.models.vgg16(pretrained=True).features[:4].eval())\n",
        "        blocks.append(torchvision.models.vgg16(pretrained=True).features[4:9].eval())\n",
        "        blocks.append(torchvision.models.vgg16(pretrained=True).features[9:16].eval())\n",
        "        blocks.append(torchvision.models.vgg16(pretrained=True).features[16:23].eval())\n",
        "        for bl in blocks:\n",
        "            for p in bl.parameters():\n",
        "                p.requires_grad = False\n",
        "        self.blocks = torch.nn.ModuleList(blocks)\n",
        "        self.transform = torch.nn.functional.interpolate\n",
        "        self.resize = resize\n",
        "        self.register_buffer(\"mean\", torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1))\n",
        "        self.register_buffer(\"std\", torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1))\n",
        "\n",
        "    def forward(self, input, target, feature_layers=[0, 1, 2, 3], style_layers=[]):\n",
        "        if input.shape[1] != 3:\n",
        "            input = input.repeat(1, 3, 1, 1)\n",
        "            target = target.repeat(1, 3, 1, 1)\n",
        "        input = (input-self.mean) / self.std\n",
        "        target = (target-self.mean) / self.std\n",
        "        if self.resize:\n",
        "            input = self.transform(input, mode='bilinear', size=(224, 224), align_corners=False)\n",
        "            target = self.transform(target, mode='bilinear', size=(224, 224), align_corners=False)\n",
        "        loss = 0.0\n",
        "        x = input\n",
        "        y = target\n",
        "        for i, block in enumerate(self.blocks):\n",
        "            x = block(x)\n",
        "            y = block(y)\n",
        "            if i in feature_layers:\n",
        "                loss += torch.nn.functional.l1_loss(x, y)\n",
        "            if i in style_layers:\n",
        "                act_x = x.reshape(x.shape[0], x.shape[1], -1)\n",
        "                act_y = y.reshape(y.shape[0], y.shape[1], -1)\n",
        "                gram_x = act_x @ act_x.permute(0, 2, 1)\n",
        "                gram_y = act_y @ act_y.permute(0, 2, 1)\n",
        "                loss += torch.nn.functional.l1_loss(gram_x, gram_y)\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdL8TDvvd8e_"
      },
      "source": [
        "#train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1eG3KU-DdejZ"
      },
      "outputs": [],
      "source": [
        "!pip install lpips"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48GUCWebQaQr"
      },
      "outputs": [],
      "source": [
        "import argparse, os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import lpips\n",
        "import skimage.color as sc\n",
        "import random\n",
        "from collections import OrderedDict\n",
        "\n",
        "\n",
        "# Training settings\n",
        "parser = argparse.ArgumentParser(description=\"IMDN\")\n",
        "parser.add_argument(\"--batch_size\", type=int, default=16,\n",
        "                    help=\"training batch size\")#16\n",
        "parser.add_argument(\"--testBatchSize\", type=int, default=1,\n",
        "                    help=\"testing batch size\")\n",
        "parser.add_argument(\"-nEpochs\", type=int, default=100,\n",
        "                    help=\"number of epochs to train\")#1000\n",
        "parser.add_argument(\"--lr\", type=float, default=2e-4,\n",
        "                    help=\"Learning Rate. Default=2e-4\")\n",
        "parser.add_argument(\"--step_size\", type=int, default=200,\n",
        "                    help=\"learning rate decay per N epochs\")#200\n",
        "parser.add_argument(\"--gamma\", type=int, default=0.5,\n",
        "                    help=\"learning rate decay factor for step decay\")\n",
        "parser.add_argument(\"--cuda\", action=\"store_true\", default=True,\n",
        "                    help=\"use cuda\")\n",
        "parser.add_argument(\"--resume\", default=\"\", type=str,\n",
        "                    help=\"path to checkpoint\")\n",
        "parser.add_argument(\"--start-epoch\", default=1, type=int,\n",
        "                    help=\"manual epoch number\")\n",
        "parser.add_argument(\"--threads\", type=int, default=8,\n",
        "                    help=\"number of threads for data loading\")#8\n",
        "parser.add_argument(\"--root\", type=str, default=\"./drive/MyDrive/dataset\",\n",
        "                    help='dataset directory')#training_data/\n",
        "parser.add_argument(\"--n_train\", type=int, default=800,\n",
        "                    help=\"number of training set\")#800\n",
        "parser.add_argument(\"--n_val\", type=int, default=1,\n",
        "                    help=\"number of validation set\")\n",
        "parser.add_argument(\"--test_every\", type=int, default=1000)\n",
        "parser.add_argument(\"--scale\", type=int, default=2,\n",
        "                    help=\"super-resolution scale\")#2,3,4\n",
        "parser.add_argument(\"--patch_size\", type=int, default=192,\n",
        "                    help=\"output patch size\")#192 ,96 ,144\n",
        "parser.add_argument(\"--rgb_range\", type=int, default=1,\n",
        "                    help=\"maxium value of RGB\")\n",
        "parser.add_argument(\"--n_colors\", type=int, default=3,\n",
        "                    help=\"number of color channels to use\")\n",
        "parser.add_argument(\"--pretrained\", default=\"\", type=str,\n",
        "                    help=\"path to pretrained models\")\n",
        "parser.add_argument(\"--seed\", type=int, default=1)\n",
        "parser.add_argument(\"--isY\", action=\"store_true\", default=True)\n",
        "parser.add_argument(\"--ext\", type=str, default='.npy')\n",
        "parser.add_argument(\"--phase\", type=str, default='train')\n",
        "\n",
        "args = parser.parse_args(args = [])\n",
        "\n",
        "testset = DatasetFromFolderVal(\"/content/drive/MyDrive/Test_Datasets/Set5/Set5/X2/\",\n",
        "                               \"/content/drive/MyDrive/Test_Datasets/Set5/Set5_LR/X{}/\".format(args.scale),\n",
        "                              args.scale)\n",
        "\n",
        "testing_data_loader = DataLoader(dataset=testset, num_workers=args.threads, batch_size=args.testBatchSize,shuffle=False)\n",
        "len_testing_data= len(testing_data_loader)\n",
        "\n",
        "print(args)\n",
        "torch.backends.cudnn.benchmark = True\n",
        "# random seed\n",
        "seed = args.seed\n",
        "if seed is None:\n",
        "  seed = random.randint(1, 10000)\n",
        "\n",
        "print(\"Ramdom Seed: \", seed)\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "cuda = args.cuda\n",
        "device = torch.device('cuda' if cuda else 'cpu')\n",
        "\n",
        "print(\"===> Loading datasets\")\n",
        "\n",
        "trainset = div2k(args)\n",
        "\n",
        "training_data_loader = DataLoader(dataset=trainset, num_workers=args.threads, batch_size=args.batch_size, shuffle=True, pin_memory=True, drop_last=True)\n",
        "\n",
        "\n",
        "print(\"===> Building models\")\n",
        "args.is_train = True\n",
        "\n",
        "model = IMDN(upscale=args.scale)\n",
        "l1_criterion = nn.L1Loss()\n",
        "vgg_criterion = VGGPerceptualLoss()\n",
        "\n",
        "print(\"===> Setting GPU\")\n",
        "if cuda:\n",
        "  model = model.to(device)\n",
        "  l1_criterion = l1_criterion.to(device)\n",
        "  vgg_criterion = vgg_criterion.to(device)\n",
        "\n",
        "if args.pretrained:\n",
        "  if os.path.isfile(args.pretrained):\n",
        "    print(\"===> loading models '{}'\".format(args.pretrained))\n",
        "    checkpoint = torch.load(args.pretrained)\n",
        "    new_state_dcit = OrderedDict()\n",
        "    for k, v in checkpoint.items():\n",
        "      if 'module' in k:\n",
        "        name = k[7:]\n",
        "      else:\n",
        "        name = k\n",
        "      new_state_dcit[name] = v\n",
        "    model_dict = model.state_dict()\n",
        "    pretrained_dict = {k: v for k, v in new_state_dcit.items() if k in model_dict}\n",
        "\n",
        "    for k, v in model_dict.items():\n",
        "      if k not in pretrained_dict:\n",
        "        print(k)\n",
        "    model.load_state_dict(pretrained_dict, strict=True)\n",
        "\n",
        "  else:\n",
        "    print(\"===> no models found at '{}'\".format(args.pretrained))\n",
        "\n",
        "print(\"===> Setting Optimizer\")\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
        "\n",
        "\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    adjust_learning_rate(optimizer, epoch, args.step_size, args.lr, args.gamma)\n",
        "    print('epoch =', epoch, 'lr = ', optimizer.param_groups[0]['lr'])\n",
        "    for iteration, (lr_tensor, hr_tensor) in enumerate(training_data_loader, 1):\n",
        "\n",
        "        if args.cuda:\n",
        "            lr_tensor = lr_tensor.to(device)  # ranges from [0, 1]\n",
        "            hr_tensor = hr_tensor.to(device)  # ranges from [0, 1]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        sr_tensor = model(lr_tensor)\n",
        "        loss_l1 = l1_criterion(sr_tensor, hr_tensor) + 0.008*vgg_criterion(sr_tensor, hr_tensor)\n",
        "        loss_sr = loss_l1\n",
        "\n",
        "        loss_sr.backward()\n",
        "        optimizer.step()\n",
        "        if iteration % 100 == 0:\n",
        "            print(\"===> Epoch[{}]({}/{}): Loss_l1: {:.5f}\".format(epoch, iteration, len(training_data_loader),\n",
        "                                                                  loss_l1.item()))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "valid_vgg = lpips.LPIPS(net='vgg')\n",
        "valid_alex = lpips.LPIPS(net='alex')\n",
        "\n",
        "\n",
        "def valid():\n",
        "    model.eval()\n",
        "\n",
        "    avg_psnr, avg_ssim , avg_lpips_vgg , avg_lpips_alex= 0, 0 ,0 , 0\n",
        "    for batch in testing_data_loader:\n",
        "        lr_tensor, hr_tensor = batch[0], batch[1]\n",
        "        if args.cuda:\n",
        "            lr_tensor = lr_tensor.to(device)\n",
        "            hr_tensor = hr_tensor.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            pre = model(lr_tensor)\n",
        "\n",
        "        sr_img = tensor2np(pre.detach()[0])\n",
        "        gt_img = tensor2np(hr_tensor.detach()[0])\n",
        "        crop_size = args.scale\n",
        "        cropped_sr_img = shave(sr_img, crop_size)\n",
        "        cropped_gt_img = shave(gt_img, crop_size)\n",
        "        if args.isY is True:\n",
        "            im_label = quantize(sc.rgb2ycbcr(cropped_gt_img)[:, :, 0])\n",
        "            im_pre = quantize(sc.rgb2ycbcr(cropped_sr_img)[:, :, 0])\n",
        "        else:\n",
        "            im_label = cropped_gt_img\n",
        "            im_pre = cropped_sr_img\n",
        "        avg_psnr += compute_psnr(im_pre, im_label)\n",
        "        avg_ssim += compute_ssim(im_pre, im_label)\n",
        "\n",
        "        avg_lpips_alex += valid_alex(torch.tensor(im_pre).float() , torch.tensor(im_label).float() )\n",
        "        avg_lpips_vgg += valid_vgg(torch.tensor(im_pre).float() , torch.tensor(im_label).float() )\n",
        "\n",
        "    print(\"===> Valid. psnr: {:.4f}, ssim: {:.4f} \".format(avg_psnr / len_testing_data,\n",
        "                                                           avg_ssim /len_testing_data ),\n",
        "           \"lpips_vgg: \" , avg_lpips_vgg.item() / len_testing_data,\n",
        "          \"/t lpips_alex: \" , avg_lpips_alex.item() / len_testing_data)\n",
        "\n",
        "\n",
        "\n",
        "def save_checkpoint(epoch):\n",
        "  model_folder = \"checkpoint_x{}/\".format(args.scale)\n",
        "  model_out_path = model_folder + \"epoch_{}.pth\".format(epoch)\n",
        "  if not os.path.exists(model_folder):\n",
        "    os.makedirs(model_folder)\n",
        "  torch.save(model.state_dict(), model_out_path)\n",
        "  print(\"===> Checkpoint saved to {}\".format(model_out_path))\n",
        "\n",
        "def print_network(net):\n",
        "  num_params = 0\n",
        "  for param in net.parameters():\n",
        "    num_params += param.numel()\n",
        "  print(net)\n",
        "  print('Total number of parameters: %d' % num_params)\n",
        "\n",
        "\n",
        "print(\"===> Training\")\n",
        "print_network(model)\n",
        "\n",
        "for epoch in range(args.start_epoch, args.nEpochs + 1):\n",
        "  train(epoch)\n",
        "  valid()\n",
        "\n",
        "  save_checkpoint(epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmstHyAHAypW"
      },
      "source": [
        "# test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1L1BAJsBbil"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import torch\n",
        "import os\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "import skimage.color as sc\n",
        "import cv2\n",
        "import lpips\n",
        "\n",
        "# Testing settings\n",
        "\n",
        "parser = argparse.ArgumentParser(description='IMDN')\n",
        "parser.add_argument(\"--test_hr_folder\", type=str, default='/content/drive/MyDrive/Test_Datasets/Set5/Set5/X2/',\n",
        "                    help='the folder of the target images')\n",
        "parser.add_argument(\"--test_lr_folder\", type=str, default='/content/drive/MyDrive/Test_Datasets/Set5/Set5_LR/X2/',\n",
        "                    help='the folder of the input images')\n",
        "parser.add_argument(\"--output_folder\", type=str, default='/content/drive/MyDrive/IMDN/checkpoint_x2/deep invo/ressult')\n",
        "parser.add_argument(\"--checkpoint\", type=str, default='/content/drive/MyDrive/IMDN/checkpoint_x2/deep invo/epoch_100.pth',\n",
        "                    help='checkpoint folder to use')\n",
        "parser.add_argument('--cuda', action='store_true', default=True,\n",
        "                    help='use cuda')\n",
        "parser.add_argument(\"--isY\", action=\"store_true\", default=True)\n",
        "parser.add_argument(\"--upscale_factor\", type=int, default=4,\n",
        "                    help='upscaling factor')\n",
        "parser.add_argument(\"--is_y\", action='store_true', default=True,\n",
        "                    help='evaluate on y channel, if False evaluate on RGB channels')\n",
        "opt = parser.parse_args(args=[])\n",
        "\n",
        "print(opt)\n",
        "\n",
        "\n",
        "\n",
        "testset = DatasetFromFolderVal(\"/content/drive/MyDrive/Test_Datasets/Set5/Set5/X2\",\n",
        "                               \"/content/drive/MyDrive/Test_Datasets/Set5/Set5_LR/X{}/\".format(args.scale),\n",
        "                              args.scale)\n",
        "\n",
        "testing_data_loader = DataLoader(dataset=testset, num_workers=args.threads, batch_size=args.testBatchSize,shuffle=False)\n",
        "len_testing_data= len(testing_data_loader)\n",
        "\n",
        "print(len_testing_data)\n",
        "\n",
        "\n",
        "\n",
        "cuda = opt.cuda\n",
        "device = torch.device('cuda' if cuda else 'cpu')\n",
        "\n",
        "filepath = opt.test_hr_folder\n",
        "if filepath.split('/')[-3] == 'Set5' or filepath.split('/')[-3] == 'Set14' or filepath.split('/')[-3] == 'BSDS100' or filepath.split('/')[-3] == 'Urban100':\n",
        "  ext = '.png'\n",
        "else:\n",
        "  ext = '.bmp'\n",
        "filelist = get_list(filepath, ext=ext)\n",
        "print(filelist)\n",
        "\n",
        "\n",
        "\n",
        "valid_vgg = lpips.LPIPS(net='vgg')\n",
        "valid_alex = lpips.LPIPS(net='alex')\n",
        "\n",
        "\n",
        "\n",
        "psnr_list = np.zeros(len(testing_data_loader))\n",
        "ssim_list = np.zeros(len(testing_data_loader))\n",
        "lpips_list_vgg = np.zeros(len(testing_data_loader))\n",
        "lpips_list_alex = np.zeros(len(testing_data_loader))\n",
        "time_list = np.zeros(len(testing_data_loader))\n",
        "\n",
        "model = IMDN(upscale=2).to(device)\n",
        "model_dict =load_state_dict(opt.checkpoint)\n",
        "model.load_state_dict(model_dict, strict=True)\n",
        "\n",
        "def test():\n",
        "    model.eval()\n",
        "    i=0\n",
        "\n",
        "    avg_psnr, avg_ssim , avg_lpips_vgg , avg_lpips_alex= 0, 0 ,0 , 0\n",
        "    for batch in testing_data_loader:\n",
        "        lr_tensor, hr_tensor = batch[0], batch[1]\n",
        "        if cuda:\n",
        "            lr_tensor = lr_tensor.to(device)\n",
        "            hr_tensor = hr_tensor.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            pre = model(lr_tensor)\n",
        "\n",
        "        sr_img = tensor2np(pre.detach()[0])\n",
        "        gt_img = tensor2np(hr_tensor.detach()[0])\n",
        "        crop_size = opt.upscale_factor\n",
        "        cropped_sr_img = shave(sr_img, crop_size)\n",
        "        cropped_gt_img = shave(gt_img, crop_size)\n",
        "        if opt.isY is True:\n",
        "            im_label = quantize(sc.rgb2ycbcr(cropped_gt_img)[:, :, 0])\n",
        "            im_pre = quantize(sc.rgb2ycbcr(cropped_sr_img)[:, :, 0])\n",
        "        else:\n",
        "            im_label = cropped_gt_img\n",
        "            im_pre = cropped_sr_img\n",
        "        avg_psnr += compute_psnr(im_pre, im_label)\n",
        "        avg_ssim += compute_ssim(im_pre, im_label)\n",
        "\n",
        "        #Added avg_lpips by me\n",
        "\n",
        "        #avg_PieAPP += valid_PieAPP(torch.tensor(im_pre).float() , torch.tensor(im_label).float() )\n",
        "        avg_lpips_alex += valid_alex(torch.tensor(im_pre).float() , torch.tensor(im_label).float() )\n",
        "        avg_lpips_vgg += valid_vgg(torch.tensor(im_pre).float() , torch.tensor(im_label).float() )\n",
        "\n",
        "\n",
        "\n",
        "        output_folder = os.path.join(opt.output_folder, filelist[i].split('/')[-1].split('.')[0] + 'x' + str(opt.upscale_factor) + '.png')\n",
        "\n",
        "\n",
        "        if not os.path.exists(opt.output_folder):\n",
        "          os.makedirs(opt.output_folder)\n",
        "\n",
        "        cv2.imwrite(output_folder, sr_img[:, :, [2, 1, 0]])\n",
        "        i+=1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    print(\"===> Valid. psnr: {:.4f}, ssim: {:.4f} \".format(avg_psnr / len_testing_data,\n",
        "                                                           avg_ssim /len_testing_data ),\n",
        "           \"lpips_vgg: \" , avg_lpips_vgg.item() / len_testing_data,\n",
        "          \"/t lpips_alex: \" , avg_lpips_alex.item() / len_testing_data)\n",
        "\n",
        "\n",
        "\n",
        "test()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "Go98ttvHc96n",
        "yRHlIE_5dRxO",
        "dhQgvC9hddC-",
        "Jg72UmaSdm69",
        "fSWXt-7GRKj3",
        "qEcmFpUXdt1f",
        "VlqdWdl-d1qG",
        "xay-Lf3LYDHI",
        "KdL8TDvvd8e_",
        "mmstHyAHAypW"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNpL2tGSf0jy3TOVPRCD/0A",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}